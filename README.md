# 关键词提取的常用方法

1. tfidf: 
    - 提取的是在当前一类文档中出现的次数多，其他类别文章中出现的次数少的词语
    - 依赖其他语料
2. textrank：
    - 提取的是在window中出现的次数多的词语
    - 不依赖语料
3. lsi
    - 通多svd把词，文档映射到一个低纬的语义空间，能够挖局浅层语义信息，通过svd直接求出word-topic-document的分布信息，是一种自动降维的手段
    - 依赖语料
4. lda
    - 基于贝叶斯理论，通过概率的方式计算word-topic-document的分布，是一种自动降维的手段
    - 依赖语料


# **使用方法示例**

**修改说明：**
- 使用新的数据的到模型,原则上效果更好
- 实现的模型的保存和加载，速度提高几百倍

`keyword_extractor.extract_by_*` 方法只需要传递句子即可

```python
sentence = """悦谷百味 糙米 1kg（无添加 五谷杂粮 含胚芽 东北 粗粮 大米伴侣 粥米搭档 真空装）
【自营极速达·超值精选购翻天】限时9.9元秒杀，品质生活精致选择，点击进入悦谷百味专场》"""
#1. 实例化
keyword_extractor = KeyWordExtractor(topK=7)
print(sentence)

#2. 使用tfidf提取关键词
ret = keyword_extractor.extract_by_tfidf(sentence)
print("tfidf:",ret)

#3. 使用textrank提取关键词，window--》窗口的大小，word_min_len——》最小词语的数量
ret = keyword_extractor.extract_by_textrank(sentence)
print("textrank:",ret)

#4. 使用lsi提取关键词，现有语料的topic的数量
ret = keyword_extractor.extract_by_lsi(sentence)
print("lsi:",ret)

#5. 使用lda提取关键词，现有语料的topic的数量
ret = keyword_extractor.extract_by_lda(sentence)
print("lda",ret)
```

### 返回值

不同方法返回结果过为列表，列表中元素为长度为2的列表，
第0个元素是词，第1个元素是值，比如tfidf的值，textrankd额值等等，可以理解为权重

```python
悦谷百味 糙米 1kg（无添加 五谷杂粮 含胚芽 东北 粗粮 大米伴侣 粥米搭档 真空装）
【自营极速达·超值精选购翻天】限时9.9元秒杀，品质生活精致选择，点击进入悦谷百味专场》

tfidf: [['百味', 0.5849063660317286], ['糙米', 0.2924531830158643], ['无添加', 0.2924531830158643], ['五谷杂粮', 0.2924531830158643], ['胚芽', 0.2924531830158643], ['粗粮', 0.2924531830158643], ['伴侣', 0.2924531830158643]]

textrank: [('无添加', 1.0), ('胚芽', 0.9601907655014996), ('五谷杂粮', 0.9562972797669131), ('东北', 0.9477637278279213), ('伴侣', 0.8529083128231151), ('粗粮', 0.7787515862908918), ('限时', 0.7520929830444977)]

lsi: [['秒杀', 1.6607436018060937], ['限时', 1.6510849178642866], ['品质', 0.4521311526624584], ['专场', 0.24820396625845867], ['东北', 0.08015567441698682]]

lda [['东北', 1.1522559249551034], ['专场', 1.1522487249358626], ['限时', 1.1522389842149763], ['秒杀', 1.1521651804637474], ['品质', 1.1425438065553175]]
```

### 返回结果分析
- tfidf和textrank的结果较好
- lsi和lda的结果较差，原因在现在的商品的一级标题二级标题中有大量类似`秒杀`，`专场`等词语，而lsi和lda是在计算和现有的6个主体最相似的词语，所以把这些相似的提取出来的


# tovec使用示例

添加的新的方法，**使用方法已经修改**

### 获取词向量

```python
from tovec import doc2vec_model,word2vec_model,doc_similarity,doc_infer_vector

#1. 获取词向量
word = "手表"
model = word2vec_model()
print("{} 的vector是：{}\n".format(word,model[word]))  #：[ 0.1789252   0.49610293 -0.35405022 ...] 100列
```
### 返回值结果分析

返回向量100维

```
手表 的vector是：[ 0.1789252   0.49610293 -0.35405022  0.36188853  0.53748924 -0.45974827
...0.6318348   0.40175536 -0.8818919   0.14242755] 

```
### 获取文章（句子）向量和文章（句子)的相似度

```python
s1 = """悦谷百味 糙米 1kg（无添加 五谷杂粮 含胚芽 东北 粗粮 大米伴侣 粥米搭档 真空装）
        【自营极速达·超值精选购翻天】限时9.9元秒杀，品质生活精致选择，点击进入悦谷百味专场》"""
s2 = """手机","手机","手机通讯","华为 HUAWEI P10 Plus 6GB+64GB 钻雕金 移动联通电信4G手机 双卡双待","wifi双天线设计！徕卡人像摄影！P10徕卡双摄拍照，低至2988元！"""
s3 = """"手机","手机","手机通讯","Apple iPhone 8 Plus (A1864) 256GB 银色 移动联通电信4G手机","选【移动优惠购】新机配新卡，198优质靓号，流量不限量！"""

#1. 获取文档向量
model = doc2vec_model() #加载模型
doc_vec = doc_infer_vector(model,s1) #获取向量
print(doc_vec)  #[-0.08688905 -0.02424988 -0.146502...] 100列

#2. 获取文档相似度
#s1是食品，s2是华为手机
print("s1和s2的相似度为：",doc_similarity(model,s1,s2)) #s1和s2的相似度为： 0.33631673

#s2是华为手机，s3是苹果手机
print("s2和s3的相似度为：",doc_similarity(model,s3,s2)) #s2和s3的相似度为： 0.62955505
```


返回值

```
[-0.08688905 -0.02424988 -0.146502    0.3619086  -0.77003765 -0.18221354
.... -0.2899374  -0.05978084  0.1869304  -0.3972418 ]
s1和s2的相似度为： 0.33631673
s2和s3的相似度为： 0.62955505
```

### 返回结果分析

- s1句子是关于食品，s2句子是关于华为手机,相似度为0.33

- s2句子是关于华为手机，s3句子是关于苹果手机，相似度为0.63